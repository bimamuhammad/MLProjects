{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "\n",
    "int2char = (dict(enumerate(chars)))\n",
    "char2int = {ch: ind for ind, ch in int2char.items()}\n",
    "encodedText = np.array([char2int.get(char) for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr is the array to encode\n",
    "# n_labels is the number of elements to use for each encoding.\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()]=1\n",
    "\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decoding(arr):\n",
    "    flattended_array = np.nonzero(one_hot)[2]\n",
    "    decoded_array = flattended_array.reshape(arr.shape[0], -1)\n",
    "    return decoded_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6 27 72 78 22 11 17 64  2 63]\n",
      " [21 47 32 64 22 27 72 22 64 72]\n",
      " [21 47 32 64 22 27 72 22 64 72]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = np.array( [[ 6,27,72,78,22,11,17,64, 2, 63],\n",
    " [21, 47, 32, 64, 22, 27, 72, 22, 64, 72],\n",
    " [21, 47, 32, 64, 22, 27, 72, 22, 64, 72]])\n",
    "one_hot = one_hot_encode(test_seq, 100)\n",
    "\n",
    "print(one_hot_decoding(one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size, seq_length):\n",
    "    n_batches = len(data) // (batch_size*seq_length)\n",
    "\n",
    "    data = data[:(n_batches*batch_size*seq_length)]\n",
    "    data = data.reshape((batch_size, -1))\n",
    "\n",
    "    for n in range(0, data.shape[1], seq_length):\n",
    "        x = data[:,n:n+seq_length]\n",
    "        y = data[:,n+1:n+seq_length+1]\n",
    "        if x.shape != y.shape:\n",
    "            continue\n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLSTM(nn.Module):\n",
    "    def __init__(self, tokens, hidden_dim, n_layers, dropout):\n",
    "\n",
    "        super().__init__()\n",
    "        self.tokens = tokens\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = len(tokens),\n",
    "            hidden_size = self.hidden_dim,\n",
    "            num_layers=self.n_layers,\n",
    "            dropout=self.dropout,\n",
    "            batch_first=True\n",
    "            )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        self.fc = nn.Linear(\n",
    "            self.hidden_dim,\n",
    "            len(tokens)\n",
    "            )\n",
    "\n",
    "            \n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        r_out, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_out)\n",
    "\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def initialize_hidden(self, batch_size):\n",
    "        weights = next(self.parameters()).data\n",
    "\n",
    "        return (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "        weights.new(self.n_layers, batch_size, self.hidden_dim).zero_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharacterLSTM(\n",
      "  (lstm): LSTM(83, 132, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc): Linear(in_features=132, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size=32\n",
    "hidden_dim=132\n",
    "output_dim=32\n",
    "n_layers=2\n",
    "dropout=0.1\n",
    "test_lstm = CharacterLSTM(chars, hidden_dim, n_layers, dropout)\n",
    "print(test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lstm.initialize_hidden(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, hidden_dim, batch_size, seq_length, n_chars, n_layers, dropout, lr, epochs, print_every=10, val_frac=0.1):\n",
    "    model = CharacterLSTM(chars, hidden_dim, n_layers, dropout)\n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    val_idx= int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "\n",
    "    counter=0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        hidden = model.initialize_hidden(batch_size)\n",
    "        losses = []\n",
    "        model.train()\n",
    "\n",
    "        for x,y in get_batch(data, batch_size, seq_length):\n",
    "            counter +=1\n",
    "            x=one_hot_encode(x, n_chars)\n",
    "\n",
    "            inputs, targets =  torch.from_numpy(x), torch.from_numpy(y)\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "\n",
    "            model.zero_grad()\n",
    "            pred, hidden = model(inputs, hidden)\n",
    "\n",
    "            loss = criterion(pred, targets.reshape(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optim.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                model.eval()\n",
    "                val_h = model.initialize_hidden(batch_size)\n",
    "                losses = []\n",
    "                for eval_x, eval_y in get_batch(val_data, batch_size, seq_length):\n",
    "                    eval_x = one_hot_encode(eval_x, n_chars)\n",
    "\n",
    "                    inputs, targets =  torch.from_numpy(eval_x), torch.from_numpy(eval_y)\n",
    "\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    out, val_h = model(inputs, val_h)\n",
    "\n",
    "                    vloss = criterion(out, targets.reshape(-1).long())\n",
    "\n",
    "                    losses.append(vloss.item())\n",
    "\n",
    "                print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(losses)))\n",
    "                model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.1639... Val Loss: 3.1365\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1186... Val Loss: 3.1277\n",
      "Epoch: 1/20... Step: 30... Loss: 3.0989... Val Loss: 3.1191\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1042... Val Loss: 3.1187\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1064... Val Loss: 3.1154\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1286... Val Loss: 3.1116\n",
      "Epoch: 1/20... Step: 70... Loss: 3.0930... Val Loss: 3.1030\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1025... Val Loss: 3.0821\n",
      "Epoch: 1/20... Step: 90... Loss: 3.0538... Val Loss: 3.0256\n",
      "Epoch: 1/20... Step: 100... Loss: 2.9201... Val Loss: 2.9347\n",
      "Epoch: 1/20... Step: 110... Loss: 2.8194... Val Loss: 2.8072\n",
      "Epoch: 1/20... Step: 120... Loss: 2.7251... Val Loss: 2.7015\n",
      "Epoch: 1/20... Step: 130... Loss: 2.6798... Val Loss: 2.6383\n",
      "Epoch: 1/20... Step: 140... Loss: 2.6009... Val Loss: 2.5927\n",
      "Epoch: 1/20... Step: 150... Loss: 2.5383... Val Loss: 2.5239\n",
      "Epoch: 1/20... Step: 160... Loss: 2.4609... Val Loss: 2.5047\n",
      "Epoch: 1/20... Step: 170... Loss: 2.4766... Val Loss: 2.4650\n",
      "Epoch: 1/20... Step: 180... Loss: 2.4738... Val Loss: 2.4332\n",
      "Epoch: 1/20... Step: 190... Loss: 2.4086... Val Loss: 2.4075\n",
      "Epoch: 2/20... Step: 200... Loss: 2.3975... Val Loss: 2.3819\n",
      "Epoch: 2/20... Step: 210... Loss: 2.3598... Val Loss: 2.3532\n",
      "Epoch: 2/20... Step: 220... Loss: 2.3516... Val Loss: 2.3315\n",
      "Epoch: 2/20... Step: 230... Loss: 2.3271... Val Loss: 2.3003\n",
      "Epoch: 2/20... Step: 240... Loss: 2.3008... Val Loss: 2.2699\n",
      "Epoch: 2/20... Step: 250... Loss: 2.2250... Val Loss: 2.2497\n",
      "Epoch: 2/20... Step: 260... Loss: 2.2494... Val Loss: 2.2189\n",
      "Epoch: 2/20... Step: 270... Loss: 2.2421... Val Loss: 2.1989\n",
      "Epoch: 2/20... Step: 280... Loss: 2.1983... Val Loss: 2.1725\n",
      "Epoch: 2/20... Step: 290... Loss: 2.1846... Val Loss: 2.1510\n",
      "Epoch: 2/20... Step: 300... Loss: 2.1279... Val Loss: 2.1284\n",
      "Epoch: 2/20... Step: 310... Loss: 2.1076... Val Loss: 2.1060\n",
      "Epoch: 2/20... Step: 320... Loss: 2.1538... Val Loss: 2.0887\n",
      "Epoch: 2/20... Step: 330... Loss: 2.1214... Val Loss: 2.0737\n",
      "Epoch: 2/20... Step: 340... Loss: 2.1007... Val Loss: 2.0526\n",
      "Epoch: 2/20... Step: 350... Loss: 2.0549... Val Loss: 2.0328\n",
      "Epoch: 2/20... Step: 360... Loss: 2.0498... Val Loss: 2.0166\n",
      "Epoch: 2/20... Step: 370... Loss: 2.0406... Val Loss: 2.0058\n",
      "Epoch: 2/20... Step: 380... Loss: 2.0202... Val Loss: 1.9861\n",
      "Epoch: 2/20... Step: 390... Loss: 1.9972... Val Loss: 1.9687\n",
      "Epoch: 3/20... Step: 400... Loss: 1.9860... Val Loss: 1.9499\n",
      "Epoch: 3/20... Step: 410... Loss: 1.9512... Val Loss: 1.9350\n",
      "Epoch: 3/20... Step: 420... Loss: 1.9591... Val Loss: 1.9233\n",
      "Epoch: 3/20... Step: 430... Loss: 1.9048... Val Loss: 1.9122\n",
      "Epoch: 3/20... Step: 440... Loss: 1.8957... Val Loss: 1.8965\n",
      "Epoch: 3/20... Step: 450... Loss: 1.9360... Val Loss: 1.8892\n",
      "Epoch: 3/20... Step: 460... Loss: 1.9290... Val Loss: 1.8786\n",
      "Epoch: 3/20... Step: 470... Loss: 1.8896... Val Loss: 1.8606\n",
      "Epoch: 3/20... Step: 480... Loss: 1.8820... Val Loss: 1.8532\n",
      "Epoch: 3/20... Step: 490... Loss: 1.9002... Val Loss: 1.8407\n",
      "Epoch: 3/20... Step: 500... Loss: 1.8688... Val Loss: 1.8272\n",
      "Epoch: 3/20... Step: 510... Loss: 1.8413... Val Loss: 1.8202\n",
      "Epoch: 3/20... Step: 520... Loss: 1.8390... Val Loss: 1.8090\n",
      "Epoch: 3/20... Step: 530... Loss: 1.8024... Val Loss: 1.8013\n",
      "Epoch: 3/20... Step: 540... Loss: 1.8529... Val Loss: 1.7917\n",
      "Epoch: 3/20... Step: 550... Loss: 1.8475... Val Loss: 1.7867\n",
      "Epoch: 3/20... Step: 560... Loss: 1.8351... Val Loss: 1.7805\n",
      "Epoch: 3/20... Step: 570... Loss: 1.8036... Val Loss: 1.7679\n",
      "Epoch: 3/20... Step: 580... Loss: 1.7926... Val Loss: 1.7618\n",
      "Epoch: 3/20... Step: 590... Loss: 1.7779... Val Loss: 1.7507\n",
      "Epoch: 4/20... Step: 600... Loss: 1.7808... Val Loss: 1.7421\n",
      "Epoch: 4/20... Step: 610... Loss: 1.7978... Val Loss: 1.7367\n",
      "Epoch: 4/20... Step: 620... Loss: 1.7348... Val Loss: 1.7363\n",
      "Epoch: 4/20... Step: 630... Loss: 1.7747... Val Loss: 1.7243\n",
      "Epoch: 4/20... Step: 640... Loss: 1.7930... Val Loss: 1.7162\n",
      "Epoch: 4/20... Step: 650... Loss: 1.7922... Val Loss: 1.7098\n",
      "Epoch: 4/20... Step: 660... Loss: 1.7492... Val Loss: 1.7022\n",
      "Epoch: 4/20... Step: 670... Loss: 1.7441... Val Loss: 1.7027\n",
      "Epoch: 4/20... Step: 680... Loss: 1.7258... Val Loss: 1.6964\n",
      "Epoch: 4/20... Step: 690... Loss: 1.7423... Val Loss: 1.6849\n",
      "Epoch: 4/20... Step: 700... Loss: 1.7013... Val Loss: 1.6783\n",
      "Epoch: 4/20... Step: 710... Loss: 1.7322... Val Loss: 1.6767\n",
      "Epoch: 4/20... Step: 720... Loss: 1.6929... Val Loss: 1.6701\n",
      "Epoch: 4/20... Step: 730... Loss: 1.6908... Val Loss: 1.6654\n",
      "Epoch: 4/20... Step: 740... Loss: 1.6838... Val Loss: 1.6598\n",
      "Epoch: 4/20... Step: 750... Loss: 1.6867... Val Loss: 1.6537\n",
      "Epoch: 4/20... Step: 760... Loss: 1.6651... Val Loss: 1.6492\n",
      "Epoch: 4/20... Step: 770... Loss: 1.6538... Val Loss: 1.6450\n",
      "Epoch: 4/20... Step: 780... Loss: 1.6594... Val Loss: 1.6374\n",
      "Epoch: 5/20... Step: 790... Loss: 1.6553... Val Loss: 1.6368\n",
      "Epoch: 5/20... Step: 800... Loss: 1.6547... Val Loss: 1.6298\n",
      "Epoch: 5/20... Step: 810... Loss: 1.6487... Val Loss: 1.6269\n",
      "Epoch: 5/20... Step: 820... Loss: 1.6235... Val Loss: 1.6208\n",
      "Epoch: 5/20... Step: 830... Loss: 1.6230... Val Loss: 1.6213\n",
      "Epoch: 5/20... Step: 840... Loss: 1.6628... Val Loss: 1.6136\n",
      "Epoch: 5/20... Step: 850... Loss: 1.6413... Val Loss: 1.6102\n",
      "Epoch: 5/20... Step: 860... Loss: 1.6535... Val Loss: 1.6065\n",
      "Epoch: 5/20... Step: 870... Loss: 1.6482... Val Loss: 1.6018\n",
      "Epoch: 5/20... Step: 880... Loss: 1.6088... Val Loss: 1.6007\n",
      "Epoch: 5/20... Step: 890... Loss: 1.6447... Val Loss: 1.5955\n",
      "Epoch: 5/20... Step: 900... Loss: 1.6267... Val Loss: 1.5948\n",
      "Epoch: 5/20... Step: 910... Loss: 1.6184... Val Loss: 1.5858\n",
      "Epoch: 5/20... Step: 920... Loss: 1.6302... Val Loss: 1.5836\n",
      "Epoch: 5/20... Step: 930... Loss: 1.6506... Val Loss: 1.5800\n",
      "Epoch: 5/20... Step: 940... Loss: 1.6086... Val Loss: 1.5768\n",
      "Epoch: 5/20... Step: 950... Loss: 1.5958... Val Loss: 1.5762\n",
      "Epoch: 5/20... Step: 960... Loss: 1.5961... Val Loss: 1.5715\n",
      "Epoch: 5/20... Step: 970... Loss: 1.6384... Val Loss: 1.5665\n",
      "Epoch: 5/20... Step: 980... Loss: 1.5687... Val Loss: 1.5606\n",
      "Epoch: 6/20... Step: 990... Loss: 1.5824... Val Loss: 1.5611\n",
      "Epoch: 6/20... Step: 1000... Loss: 1.5578... Val Loss: 1.5589\n",
      "Epoch: 6/20... Step: 1010... Loss: 1.5882... Val Loss: 1.5541\n",
      "Epoch: 6/20... Step: 1020... Loss: 1.5811... Val Loss: 1.5497\n",
      "Epoch: 6/20... Step: 1030... Loss: 1.5840... Val Loss: 1.5475\n",
      "Epoch: 6/20... Step: 1040... Loss: 1.5904... Val Loss: 1.5466\n",
      "Epoch: 6/20... Step: 1050... Loss: 1.5800... Val Loss: 1.5427\n",
      "Epoch: 6/20... Step: 1060... Loss: 1.6157... Val Loss: 1.5429\n",
      "Epoch: 6/20... Step: 1070... Loss: 1.5950... Val Loss: 1.5355\n",
      "Epoch: 6/20... Step: 1080... Loss: 1.5789... Val Loss: 1.5359\n",
      "Epoch: 6/20... Step: 1090... Loss: 1.5332... Val Loss: 1.5313\n",
      "Epoch: 6/20... Step: 1100... Loss: 1.5346... Val Loss: 1.5326\n",
      "Epoch: 6/20... Step: 1110... Loss: 1.5731... Val Loss: 1.5303\n",
      "Epoch: 6/20... Step: 1120... Loss: 1.5838... Val Loss: 1.5267\n",
      "Epoch: 6/20... Step: 1130... Loss: 1.5186... Val Loss: 1.5216\n",
      "Epoch: 6/20... Step: 1140... Loss: 1.5679... Val Loss: 1.5209\n",
      "Epoch: 6/20... Step: 1150... Loss: 1.5621... Val Loss: 1.5161\n",
      "Epoch: 6/20... Step: 1160... Loss: 1.5362... Val Loss: 1.5164\n",
      "Epoch: 6/20... Step: 1170... Loss: 1.5237... Val Loss: 1.5114\n",
      "Epoch: 6/20... Step: 1180... Loss: 1.5304... Val Loss: 1.5094\n",
      "Epoch: 7/20... Step: 1190... Loss: 1.5220... Val Loss: 1.5104\n",
      "Epoch: 7/20... Step: 1200... Loss: 1.5271... Val Loss: 1.5061\n",
      "Epoch: 7/20... Step: 1210... Loss: 1.5305... Val Loss: 1.5081\n",
      "Epoch: 7/20... Step: 1220... Loss: 1.5034... Val Loss: 1.4993\n",
      "Epoch: 7/20... Step: 1230... Loss: 1.5114... Val Loss: 1.5022\n",
      "Epoch: 7/20... Step: 1240... Loss: 1.5316... Val Loss: 1.4982\n",
      "Epoch: 7/20... Step: 1250... Loss: 1.5216... Val Loss: 1.4972\n",
      "Epoch: 7/20... Step: 1260... Loss: 1.5438... Val Loss: 1.4963\n",
      "Epoch: 7/20... Step: 1270... Loss: 1.5057... Val Loss: 1.4906\n",
      "Epoch: 7/20... Step: 1280... Loss: 1.4679... Val Loss: 1.4880\n",
      "Epoch: 7/20... Step: 1290... Loss: 1.4703... Val Loss: 1.4904\n",
      "Epoch: 7/20... Step: 1300... Loss: 1.4891... Val Loss: 1.4860\n",
      "Epoch: 7/20... Step: 1310... Loss: 1.5108... Val Loss: 1.4823\n",
      "Epoch: 7/20... Step: 1320... Loss: 1.5017... Val Loss: 1.4812\n",
      "Epoch: 7/20... Step: 1330... Loss: 1.4810... Val Loss: 1.4778\n",
      "Epoch: 7/20... Step: 1340... Loss: 1.4840... Val Loss: 1.4794\n",
      "Epoch: 7/20... Step: 1350... Loss: 1.5377... Val Loss: 1.4775\n",
      "Epoch: 7/20... Step: 1360... Loss: 1.4629... Val Loss: 1.4755\n",
      "Epoch: 7/20... Step: 1370... Loss: 1.4962... Val Loss: 1.4710\n",
      "Epoch: 8/20... Step: 1380... Loss: 1.5925... Val Loss: 1.4694\n",
      "Epoch: 8/20... Step: 1390... Loss: 1.5007... Val Loss: 1.4681\n",
      "Epoch: 8/20... Step: 1400... Loss: 1.5025... Val Loss: 1.4669\n",
      "Epoch: 8/20... Step: 1410... Loss: 1.4915... Val Loss: 1.4616\n",
      "Epoch: 8/20... Step: 1420... Loss: 1.4876... Val Loss: 1.4667\n",
      "Epoch: 8/20... Step: 1430... Loss: 1.4985... Val Loss: 1.4637\n",
      "Epoch: 8/20... Step: 1440... Loss: 1.5334... Val Loss: 1.4603\n",
      "Epoch: 8/20... Step: 1450... Loss: 1.5014... Val Loss: 1.4609\n",
      "Epoch: 8/20... Step: 1460... Loss: 1.4585... Val Loss: 1.4604\n",
      "Epoch: 8/20... Step: 1470... Loss: 1.4698... Val Loss: 1.4570\n",
      "Epoch: 8/20... Step: 1480... Loss: 1.4575... Val Loss: 1.4574\n",
      "Epoch: 8/20... Step: 1490... Loss: 1.4995... Val Loss: 1.4512\n",
      "Epoch: 8/20... Step: 1500... Loss: 1.5001... Val Loss: 1.4529\n",
      "Epoch: 8/20... Step: 1510... Loss: 1.4606... Val Loss: 1.4505\n",
      "Epoch: 8/20... Step: 1520... Loss: 1.5029... Val Loss: 1.4473\n",
      "Epoch: 8/20... Step: 1530... Loss: 1.4753... Val Loss: 1.4506\n",
      "Epoch: 8/20... Step: 1540... Loss: 1.5025... Val Loss: 1.4449\n",
      "Epoch: 8/20... Step: 1550... Loss: 1.4653... Val Loss: 1.4401\n",
      "Epoch: 8/20... Step: 1560... Loss: 1.4769... Val Loss: 1.4390\n",
      "Epoch: 8/20... Step: 1570... Loss: 1.4594... Val Loss: 1.4437\n",
      "Epoch: 9/20... Step: 1580... Loss: 1.4829... Val Loss: 1.4360\n",
      "Epoch: 9/20... Step: 1590... Loss: 1.4463... Val Loss: 1.4389\n",
      "Epoch: 9/20... Step: 1600... Loss: 1.4342... Val Loss: 1.4338\n",
      "Epoch: 9/20... Step: 1610... Loss: 1.4220... Val Loss: 1.4357\n",
      "Epoch: 9/20... Step: 1620... Loss: 1.4652... Val Loss: 1.4320\n",
      "Epoch: 9/20... Step: 1630... Loss: 1.4308... Val Loss: 1.4329\n",
      "Epoch: 9/20... Step: 1640... Loss: 1.4588... Val Loss: 1.4310\n",
      "Epoch: 9/20... Step: 1650... Loss: 1.4610... Val Loss: 1.4288\n",
      "Epoch: 9/20... Step: 1660... Loss: 1.4630... Val Loss: 1.4313\n",
      "Epoch: 9/20... Step: 1670... Loss: 1.4218... Val Loss: 1.4258\n",
      "Epoch: 9/20... Step: 1680... Loss: 1.4646... Val Loss: 1.4230\n",
      "Epoch: 9/20... Step: 1690... Loss: 1.4204... Val Loss: 1.4290\n",
      "Epoch: 9/20... Step: 1700... Loss: 1.4367... Val Loss: 1.4226\n",
      "Epoch: 9/20... Step: 1710... Loss: 1.4415... Val Loss: 1.4223\n",
      "Epoch: 9/20... Step: 1720... Loss: 1.4552... Val Loss: 1.4253\n",
      "Epoch: 9/20... Step: 1730... Loss: 1.4804... Val Loss: 1.4202\n",
      "Epoch: 9/20... Step: 1740... Loss: 1.4443... Val Loss: 1.4184\n",
      "Epoch: 9/20... Step: 1750... Loss: 1.4402... Val Loss: 1.4206\n",
      "Epoch: 9/20... Step: 1760... Loss: 1.4178... Val Loss: 1.4139\n",
      "Epoch: 9/20... Step: 1770... Loss: 1.4378... Val Loss: 1.4143\n",
      "Epoch: 10/20... Step: 1780... Loss: 1.4207... Val Loss: 1.4193\n",
      "Epoch: 10/20... Step: 1790... Loss: 1.4091... Val Loss: 1.4113\n",
      "Epoch: 10/20... Step: 1800... Loss: 1.4187... Val Loss: 1.4094\n",
      "Epoch: 10/20... Step: 1810... Loss: 1.4484... Val Loss: 1.4090\n",
      "Epoch: 10/20... Step: 1820... Loss: 1.4304... Val Loss: 1.4075\n",
      "Epoch: 10/20... Step: 1830... Loss: 1.4024... Val Loss: 1.4083\n",
      "Epoch: 10/20... Step: 1840... Loss: 1.4253... Val Loss: 1.4079\n",
      "Epoch: 10/20... Step: 1850... Loss: 1.4160... Val Loss: 1.4042\n",
      "Epoch: 10/20... Step: 1860... Loss: 1.4037... Val Loss: 1.4026\n",
      "Epoch: 10/20... Step: 1870... Loss: 1.4212... Val Loss: 1.4042\n",
      "Epoch: 10/20... Step: 1880... Loss: 1.3953... Val Loss: 1.4034\n",
      "Epoch: 10/20... Step: 1890... Loss: 1.4124... Val Loss: 1.3998\n",
      "Epoch: 10/20... Step: 1900... Loss: 1.4011... Val Loss: 1.4009\n",
      "Epoch: 10/20... Step: 1910... Loss: 1.4047... Val Loss: 1.4000\n",
      "Epoch: 10/20... Step: 1920... Loss: 1.4297... Val Loss: 1.3988\n",
      "Epoch: 10/20... Step: 1930... Loss: 1.3889... Val Loss: 1.3971\n",
      "Epoch: 10/20... Step: 1940... Loss: 1.3776... Val Loss: 1.3953\n",
      "Epoch: 10/20... Step: 1950... Loss: 1.3962... Val Loss: 1.3932\n",
      "Epoch: 10/20... Step: 1960... Loss: 1.4107... Val Loss: 1.3905\n",
      "Epoch: 10/20... Step: 1970... Loss: 1.3461... Val Loss: 1.3920\n",
      "Epoch: 11/20... Step: 1980... Loss: 1.4035... Val Loss: 1.3923\n",
      "Epoch: 11/20... Step: 1990... Loss: 1.3684... Val Loss: 1.3891\n",
      "Epoch: 11/20... Step: 2000... Loss: 1.3854... Val Loss: 1.3883\n",
      "Epoch: 11/20... Step: 2010... Loss: 1.3974... Val Loss: 1.3897\n",
      "Epoch: 11/20... Step: 2020... Loss: 1.3960... Val Loss: 1.3860\n",
      "Epoch: 11/20... Step: 2030... Loss: 1.3701... Val Loss: 1.3858\n",
      "Epoch: 11/20... Step: 2040... Loss: 1.4182... Val Loss: 1.3864\n",
      "Epoch: 11/20... Step: 2050... Loss: 1.4276... Val Loss: 1.3857\n",
      "Epoch: 11/20... Step: 2060... Loss: 1.3966... Val Loss: 1.3836\n",
      "Epoch: 11/20... Step: 2070... Loss: 1.3772... Val Loss: 1.3839\n",
      "Epoch: 11/20... Step: 2080... Loss: 1.3624... Val Loss: 1.3822\n",
      "Epoch: 11/20... Step: 2090... Loss: 1.3745... Val Loss: 1.3825\n",
      "Epoch: 11/20... Step: 2100... Loss: 1.4035... Val Loss: 1.3827\n",
      "Epoch: 11/20... Step: 2110... Loss: 1.3763... Val Loss: 1.3799\n",
      "Epoch: 11/20... Step: 2120... Loss: 1.3810... Val Loss: 1.3808\n",
      "Epoch: 11/20... Step: 2130... Loss: 1.3981... Val Loss: 1.3789\n",
      "Epoch: 11/20... Step: 2140... Loss: 1.3856... Val Loss: 1.3755\n",
      "Epoch: 11/20... Step: 2150... Loss: 1.3769... Val Loss: 1.3744\n",
      "Epoch: 11/20... Step: 2160... Loss: 1.3712... Val Loss: 1.3741\n",
      "Epoch: 12/20... Step: 2170... Loss: 1.4091... Val Loss: 1.3791\n",
      "Epoch: 12/20... Step: 2180... Loss: 1.3748... Val Loss: 1.3753\n",
      "Epoch: 12/20... Step: 2190... Loss: 1.4055... Val Loss: 1.3741\n",
      "Epoch: 12/20... Step: 2200... Loss: 1.3811... Val Loss: 1.3713\n",
      "Epoch: 12/20... Step: 2210... Loss: 1.3918... Val Loss: 1.3732\n",
      "Epoch: 12/20... Step: 2220... Loss: 1.3774... Val Loss: 1.3702\n",
      "Epoch: 12/20... Step: 2230... Loss: 1.3825... Val Loss: 1.3694\n",
      "Epoch: 12/20... Step: 2240... Loss: 1.3687... Val Loss: 1.3699\n",
      "Epoch: 12/20... Step: 2250... Loss: 1.3451... Val Loss: 1.3719\n",
      "Epoch: 12/20... Step: 2260... Loss: 1.3816... Val Loss: 1.3696\n",
      "Epoch: 12/20... Step: 2270... Loss: 1.3696... Val Loss: 1.3653\n",
      "Epoch: 12/20... Step: 2280... Loss: 1.3592... Val Loss: 1.3672\n",
      "Epoch: 12/20... Step: 2290... Loss: 1.4064... Val Loss: 1.3658\n",
      "Epoch: 12/20... Step: 2300... Loss: 1.3641... Val Loss: 1.3657\n",
      "Epoch: 12/20... Step: 2310... Loss: 1.3738... Val Loss: 1.3648\n",
      "Epoch: 12/20... Step: 2320... Loss: 1.3621... Val Loss: 1.3609\n",
      "Epoch: 12/20... Step: 2330... Loss: 1.3526... Val Loss: 1.3632\n",
      "Epoch: 12/20... Step: 2340... Loss: 1.3830... Val Loss: 1.3594\n",
      "Epoch: 12/20... Step: 2350... Loss: 1.3646... Val Loss: 1.3587\n",
      "Epoch: 12/20... Step: 2360... Loss: 1.3503... Val Loss: 1.3600\n",
      "Epoch: 13/20... Step: 2370... Loss: 1.3534... Val Loss: 1.3620\n",
      "Epoch: 13/20... Step: 2380... Loss: 1.3127... Val Loss: 1.3593\n",
      "Epoch: 13/20... Step: 2390... Loss: 1.3461... Val Loss: 1.3567\n",
      "Epoch: 13/20... Step: 2400... Loss: 1.3376... Val Loss: 1.3563\n",
      "Epoch: 13/20... Step: 2410... Loss: 1.3471... Val Loss: 1.3575\n",
      "Epoch: 13/20... Step: 2420... Loss: 1.3827... Val Loss: 1.3575\n",
      "Epoch: 13/20... Step: 2430... Loss: 1.3528... Val Loss: 1.3548\n",
      "Epoch: 13/20... Step: 2440... Loss: 1.3442... Val Loss: 1.3533\n",
      "Epoch: 13/20... Step: 2450... Loss: 1.3331... Val Loss: 1.3560\n",
      "Epoch: 13/20... Step: 2460... Loss: 1.3887... Val Loss: 1.3552\n",
      "Epoch: 13/20... Step: 2470... Loss: 1.3624... Val Loss: 1.3526\n",
      "Epoch: 13/20... Step: 2480... Loss: 1.3399... Val Loss: 1.3533\n",
      "Epoch: 13/20... Step: 2490... Loss: 1.3220... Val Loss: 1.3510\n",
      "Epoch: 13/20... Step: 2500... Loss: 1.3223... Val Loss: 1.3518\n",
      "Epoch: 13/20... Step: 2510... Loss: 1.3476... Val Loss: 1.3516\n",
      "Epoch: 13/20... Step: 2520... Loss: 1.3661... Val Loss: 1.3503\n",
      "Epoch: 13/20... Step: 2530... Loss: 1.3748... Val Loss: 1.3478\n",
      "Epoch: 13/20... Step: 2540... Loss: 1.3454... Val Loss: 1.3554\n",
      "Epoch: 13/20... Step: 2550... Loss: 1.3457... Val Loss: 1.3487\n",
      "Epoch: 13/20... Step: 2560... Loss: 1.3020... Val Loss: 1.3485\n",
      "Epoch: 14/20... Step: 2570... Loss: 1.3354... Val Loss: 1.3478\n",
      "Epoch: 14/20... Step: 2580... Loss: 1.3405... Val Loss: 1.3538\n",
      "Epoch: 14/20... Step: 2590... Loss: 1.3105... Val Loss: 1.3451\n",
      "Epoch: 14/20... Step: 2600... Loss: 1.3496... Val Loss: 1.3426\n",
      "Epoch: 14/20... Step: 2610... Loss: 1.3794... Val Loss: 1.3441\n",
      "Epoch: 14/20... Step: 2620... Loss: 1.3864... Val Loss: 1.3439\n",
      "Epoch: 14/20... Step: 2630... Loss: 1.3342... Val Loss: 1.3461\n",
      "Epoch: 14/20... Step: 2640... Loss: 1.3420... Val Loss: 1.3405\n",
      "Epoch: 14/20... Step: 2650... Loss: 1.3242... Val Loss: 1.3408\n",
      "Epoch: 14/20... Step: 2660... Loss: 1.3535... Val Loss: 1.3407\n",
      "Epoch: 14/20... Step: 2670... Loss: 1.3381... Val Loss: 1.3446\n",
      "Epoch: 14/20... Step: 2680... Loss: 1.3497... Val Loss: 1.3400\n",
      "Epoch: 14/20... Step: 2690... Loss: 1.3061... Val Loss: 1.3395\n",
      "Epoch: 14/20... Step: 2700... Loss: 1.3223... Val Loss: 1.3413\n",
      "Epoch: 14/20... Step: 2710... Loss: 1.3241... Val Loss: 1.3379\n",
      "Epoch: 14/20... Step: 2720... Loss: 1.3305... Val Loss: 1.3360\n",
      "Epoch: 14/20... Step: 2730... Loss: 1.3191... Val Loss: 1.3376\n",
      "Epoch: 14/20... Step: 2740... Loss: 1.2925... Val Loss: 1.3360\n",
      "Epoch: 14/20... Step: 2750... Loss: 1.3101... Val Loss: 1.3354\n",
      "Epoch: 15/20... Step: 2760... Loss: 1.3119... Val Loss: 1.3374\n",
      "Epoch: 15/20... Step: 2770... Loss: 1.3126... Val Loss: 1.3354\n",
      "Epoch: 15/20... Step: 2780... Loss: 1.3217... Val Loss: 1.3336\n",
      "Epoch: 15/20... Step: 2790... Loss: 1.3019... Val Loss: 1.3331\n",
      "Epoch: 15/20... Step: 2800... Loss: 1.3257... Val Loss: 1.3379\n",
      "Epoch: 15/20... Step: 2810... Loss: 1.3379... Val Loss: 1.3332\n",
      "Epoch: 15/20... Step: 2820... Loss: 1.3029... Val Loss: 1.3415\n",
      "Epoch: 15/20... Step: 2830... Loss: 1.3407... Val Loss: 1.3316\n",
      "Epoch: 15/20... Step: 2840... Loss: 1.3137... Val Loss: 1.3316\n",
      "Epoch: 15/20... Step: 2850... Loss: 1.3054... Val Loss: 1.3312\n",
      "Epoch: 15/20... Step: 2860... Loss: 1.3268... Val Loss: 1.3300\n",
      "Epoch: 15/20... Step: 2870... Loss: 1.3194... Val Loss: 1.3300\n",
      "Epoch: 15/20... Step: 2880... Loss: 1.3294... Val Loss: 1.3306\n",
      "Epoch: 15/20... Step: 2890... Loss: 1.3369... Val Loss: 1.3293\n",
      "Epoch: 15/20... Step: 2900... Loss: 1.3534... Val Loss: 1.3294\n",
      "Epoch: 15/20... Step: 2910... Loss: 1.3121... Val Loss: 1.3312\n",
      "Epoch: 15/20... Step: 2920... Loss: 1.3055... Val Loss: 1.3303\n",
      "Epoch: 15/20... Step: 2930... Loss: 1.3049... Val Loss: 1.3257\n",
      "Epoch: 15/20... Step: 2940... Loss: 1.3520... Val Loss: 1.3253\n",
      "Epoch: 15/20... Step: 2950... Loss: 1.3034... Val Loss: 1.3244\n",
      "Epoch: 16/20... Step: 2960... Loss: 1.3191... Val Loss: 1.3304\n",
      "Epoch: 16/20... Step: 2970... Loss: 1.3016... Val Loss: 1.3309\n",
      "Epoch: 16/20... Step: 2980... Loss: 1.3204... Val Loss: 1.3270\n",
      "Epoch: 16/20... Step: 2990... Loss: 1.3116... Val Loss: 1.3223\n",
      "Epoch: 16/20... Step: 3000... Loss: 1.3268... Val Loss: 1.3242\n",
      "Epoch: 16/20... Step: 3010... Loss: 1.3196... Val Loss: 1.3277\n",
      "Epoch: 16/20... Step: 3020... Loss: 1.3154... Val Loss: 1.3216\n",
      "Epoch: 16/20... Step: 3030... Loss: 1.3319... Val Loss: 1.3226\n",
      "Epoch: 16/20... Step: 3040... Loss: 1.3349... Val Loss: 1.3227\n",
      "Epoch: 16/20... Step: 3050... Loss: 1.3171... Val Loss: 1.3255\n",
      "Epoch: 16/20... Step: 3060... Loss: 1.2877... Val Loss: 1.3205\n",
      "Epoch: 16/20... Step: 3070... Loss: 1.2856... Val Loss: 1.3231\n",
      "Epoch: 16/20... Step: 3080... Loss: 1.2996... Val Loss: 1.3200\n",
      "Epoch: 16/20... Step: 3090... Loss: 1.3602... Val Loss: 1.3217\n",
      "Epoch: 16/20... Step: 3100... Loss: 1.2942... Val Loss: 1.3223\n",
      "Epoch: 16/20... Step: 3110... Loss: 1.3275... Val Loss: 1.3185\n",
      "Epoch: 16/20... Step: 3120... Loss: 1.2959... Val Loss: 1.3175\n",
      "Epoch: 16/20... Step: 3130... Loss: 1.3082... Val Loss: 1.3200\n",
      "Epoch: 16/20... Step: 3140... Loss: 1.2887... Val Loss: 1.3182\n",
      "Epoch: 16/20... Step: 3150... Loss: 1.2952... Val Loss: 1.3171\n",
      "Epoch: 17/20... Step: 3160... Loss: 1.3003... Val Loss: 1.3202\n",
      "Epoch: 17/20... Step: 3170... Loss: 1.2973... Val Loss: 1.3177\n",
      "Epoch: 17/20... Step: 3180... Loss: 1.2861... Val Loss: 1.3157\n",
      "Epoch: 17/20... Step: 3190... Loss: 1.2734... Val Loss: 1.3151\n",
      "Epoch: 17/20... Step: 3200... Loss: 1.3024... Val Loss: 1.3157\n",
      "Epoch: 17/20... Step: 3210... Loss: 1.3153... Val Loss: 1.3147\n",
      "Epoch: 17/20... Step: 3220... Loss: 1.3068... Val Loss: 1.3164\n",
      "Epoch: 17/20... Step: 3230... Loss: 1.3152... Val Loss: 1.3154\n",
      "Epoch: 17/20... Step: 3240... Loss: 1.2754... Val Loss: 1.3149\n",
      "Epoch: 17/20... Step: 3250... Loss: 1.2640... Val Loss: 1.3131\n",
      "Epoch: 17/20... Step: 3260... Loss: 1.2720... Val Loss: 1.3162\n",
      "Epoch: 17/20... Step: 3270... Loss: 1.2698... Val Loss: 1.3147\n",
      "Epoch: 17/20... Step: 3280... Loss: 1.3017... Val Loss: 1.3130\n",
      "Epoch: 17/20... Step: 3290... Loss: 1.2892... Val Loss: 1.3152\n",
      "Epoch: 17/20... Step: 3300... Loss: 1.2789... Val Loss: 1.3150\n",
      "Epoch: 17/20... Step: 3310... Loss: 1.2842... Val Loss: 1.3145\n",
      "Epoch: 17/20... Step: 3320... Loss: 1.3372... Val Loss: 1.3111\n",
      "Epoch: 17/20... Step: 3330... Loss: 1.2671... Val Loss: 1.3120\n",
      "Epoch: 17/20... Step: 3340... Loss: 1.2913... Val Loss: 1.3115\n",
      "Epoch: 18/20... Step: 3350... Loss: 1.3759... Val Loss: 1.3078\n",
      "Epoch: 18/20... Step: 3360... Loss: 1.2965... Val Loss: 1.3100\n",
      "Epoch: 18/20... Step: 3370... Loss: 1.2987... Val Loss: 1.3083\n",
      "Epoch: 18/20... Step: 3380... Loss: 1.3163... Val Loss: 1.3088\n",
      "Epoch: 18/20... Step: 3390... Loss: 1.2982... Val Loss: 1.3097\n",
      "Epoch: 18/20... Step: 3400... Loss: 1.3032... Val Loss: 1.3099\n",
      "Epoch: 18/20... Step: 3410... Loss: 1.3424... Val Loss: 1.3068\n",
      "Epoch: 18/20... Step: 3420... Loss: 1.3162... Val Loss: 1.3074\n",
      "Epoch: 18/20... Step: 3430... Loss: 1.2546... Val Loss: 1.3048\n",
      "Epoch: 18/20... Step: 3440... Loss: 1.2851... Val Loss: 1.3054\n",
      "Epoch: 18/20... Step: 3450... Loss: 1.2743... Val Loss: 1.3065\n",
      "Epoch: 18/20... Step: 3460... Loss: 1.2950... Val Loss: 1.3071\n",
      "Epoch: 18/20... Step: 3470... Loss: 1.3247... Val Loss: 1.3052\n",
      "Epoch: 18/20... Step: 3480... Loss: 1.2789... Val Loss: 1.3082\n",
      "Epoch: 18/20... Step: 3490... Loss: 1.3075... Val Loss: 1.3037\n",
      "Epoch: 18/20... Step: 3500... Loss: 1.2838... Val Loss: 1.3054\n",
      "Epoch: 18/20... Step: 3510... Loss: 1.3241... Val Loss: 1.3066\n",
      "Epoch: 18/20... Step: 3520... Loss: 1.2747... Val Loss: 1.3028\n",
      "Epoch: 18/20... Step: 3530... Loss: 1.2981... Val Loss: 1.3022\n",
      "Epoch: 18/20... Step: 3540... Loss: 1.2897... Val Loss: 1.3033\n",
      "Epoch: 19/20... Step: 3550... Loss: 1.3023... Val Loss: 1.3020\n",
      "Epoch: 19/20... Step: 3560... Loss: 1.2694... Val Loss: 1.3042\n",
      "Epoch: 19/20... Step: 3570... Loss: 1.2687... Val Loss: 1.3028\n",
      "Epoch: 19/20... Step: 3580... Loss: 1.2524... Val Loss: 1.3005\n",
      "Epoch: 19/20... Step: 3590... Loss: 1.3095... Val Loss: 1.3023\n",
      "Epoch: 19/20... Step: 3600... Loss: 1.2784... Val Loss: 1.3024\n",
      "Epoch: 19/20... Step: 3610... Loss: 1.2733... Val Loss: 1.2993\n",
      "Epoch: 19/20... Step: 3620... Loss: 1.2872... Val Loss: 1.2993\n",
      "Epoch: 19/20... Step: 3630... Loss: 1.2879... Val Loss: 1.2987\n",
      "Epoch: 19/20... Step: 3640... Loss: 1.2582... Val Loss: 1.3026\n",
      "Epoch: 19/20... Step: 3650... Loss: 1.3085... Val Loss: 1.2981\n",
      "Epoch: 19/20... Step: 3660... Loss: 1.2752... Val Loss: 1.2992\n",
      "Epoch: 19/20... Step: 3670... Loss: 1.2905... Val Loss: 1.3006\n",
      "Epoch: 19/20... Step: 3680... Loss: 1.2725... Val Loss: 1.2978\n",
      "Epoch: 19/20... Step: 3690... Loss: 1.2833... Val Loss: 1.2998\n",
      "Epoch: 19/20... Step: 3700... Loss: 1.3011... Val Loss: 1.2984\n",
      "Epoch: 19/20... Step: 3710... Loss: 1.2827... Val Loss: 1.2989\n",
      "Epoch: 19/20... Step: 3720... Loss: 1.2809... Val Loss: 1.2955\n",
      "Epoch: 19/20... Step: 3730... Loss: 1.2537... Val Loss: 1.2938\n",
      "Epoch: 19/20... Step: 3740... Loss: 1.2799... Val Loss: 1.2968\n",
      "Epoch: 20/20... Step: 3750... Loss: 1.2716... Val Loss: 1.3072\n",
      "Epoch: 20/20... Step: 3760... Loss: 1.2566... Val Loss: 1.2997\n",
      "Epoch: 20/20... Step: 3770... Loss: 1.2624... Val Loss: 1.2964\n",
      "Epoch: 20/20... Step: 3780... Loss: 1.2852... Val Loss: 1.2950\n",
      "Epoch: 20/20... Step: 3790... Loss: 1.2563... Val Loss: 1.2955\n",
      "Epoch: 20/20... Step: 3800... Loss: 1.2501... Val Loss: 1.2957\n",
      "Epoch: 20/20... Step: 3810... Loss: 1.2835... Val Loss: 1.2953\n",
      "Epoch: 20/20... Step: 3820... Loss: 1.2642... Val Loss: 1.2962\n",
      "Epoch: 20/20... Step: 3830... Loss: 1.2544... Val Loss: 1.2952\n",
      "Epoch: 20/20... Step: 3840... Loss: 1.2666... Val Loss: 1.2922\n",
      "Epoch: 20/20... Step: 3850... Loss: 1.2556... Val Loss: 1.2919\n",
      "Epoch: 20/20... Step: 3860... Loss: 1.2606... Val Loss: 1.2939\n",
      "Epoch: 20/20... Step: 3870... Loss: 1.2467... Val Loss: 1.2925\n",
      "Epoch: 20/20... Step: 3880... Loss: 1.2541... Val Loss: 1.2943\n",
      "Epoch: 20/20... Step: 3890... Loss: 1.2817... Val Loss: 1.2950\n",
      "Epoch: 20/20... Step: 3900... Loss: 1.2527... Val Loss: 1.2931\n",
      "Epoch: 20/20... Step: 3910... Loss: 1.2363... Val Loss: 1.2900\n",
      "Epoch: 20/20... Step: 3920... Loss: 1.2619... Val Loss: 1.2966\n",
      "Epoch: 20/20... Step: 3930... Loss: 1.2773... Val Loss: 1.2895\n",
      "Epoch: 20/20... Step: 3940... Loss: 1.2170... Val Loss: 1.2914\n"
     ]
    }
   ],
   "source": [
    "data=encodedText\n",
    "hidden_dim=256\n",
    "seq_length=100\n",
    "batch_size=200\n",
    "n_chars=83\n",
    "n_layers=2\n",
    "dropout=0.2\n",
    "lr=0.003\n",
    "epochs=20\n",
    "print_every=10\n",
    "val_frac=0.2\n",
    "trained_model  = train(data, hidden_dim, batch_size, seq_length, n_chars, n_layers, dropout, lr, epochs, print_every, val_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_x_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': trained_model.hidden_dim,\n",
    "              'n_layers': trained_model.n_layers,\n",
    "              'state_dict': trained_model.state_dict(),\n",
    "              'tokens': trained_model.tokens}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_char, hidden, topk=1):\n",
    "    x= np.array([[char2int[test_char]]])\n",
    "    x=one_hot_encode(x, 83)\n",
    "\n",
    "    inputs = torch.from_numpy(x)\n",
    "    test_h=tuple([each.data for each in hidden])\n",
    "\n",
    "    test_out, test_h = net(inputs,test_h)\n",
    "\n",
    "    p = F.softmax(test_out, dim=1).data\n",
    "\n",
    "    pro, ind = p.topk(topk)\n",
    "    return int2char[ind.item()], test_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The conversation to her side of the same the strange of the same thing to the state of the conversation of the same thing to the starting to the conve\n"
     ]
    }
   ],
   "source": [
    "test_char='T'\n",
    "test_hidden = trained_model.initialize_hidden(1)\n",
    "iterations = 150\n",
    "pred=''\n",
    "for i in range(iterations):\n",
    "    pred+=test_char\n",
    "    test_char, test_hidden  = predict(trained_model, test_char, test_hidden)\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
