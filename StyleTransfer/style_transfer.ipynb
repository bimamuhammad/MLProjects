{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, models\n",
    "import torch.optim as optim\n",
    "\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename, shape=None):\n",
    "  image = Image.open(filename).convert('RGB')\n",
    "  size = image.size if max(image.size)<= 400 else 400\n",
    "  size=shape if shape!= None else size\n",
    "  image_transform =  transforms.Compose([transforms.Resize(size), transforms.ToTensor(),transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "  image = image_transform(image)[:3,:,:].unsqueeze(0)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert image for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(image):\n",
    "  image = image.to('cpu').clone().detach()\n",
    "  image = image.numpy().squeeze()\n",
    "  image = image.transpose(1,2,0)\n",
    "  image = image * np.array((0.229, 0.224, 0.225))+np.array((0.485, 0.456, 0.406))\n",
    "  image = image.clip(0,1)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = load_image(f'{data_dir}Personal.jpg').to(device)\n",
    "style_image = load_image(f'{data_dir}hockney.jpg', shape=content_image.shape[-2:]).to(device)\n",
    "fig, (ax1, ax2)=plt.subplots(1,2, figsize=(6,3))\n",
    "ax1.imshow(im_convert(content_image))\n",
    "ax2.imshow(im_convert(style_image))\n",
    "ax1.set_title('Content')\n",
    "ax2.set_title('Style')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG19 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg19(weights='DEFAULT').features # 'VGG19_Weights.DEFAULT\n",
    "for params in model.parameters():\n",
    "  params.requires_grad_(False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image, model):\n",
    "  layers = {\n",
    "      '0': 'conv1_1',\n",
    "      '5': 'conv2_1',\n",
    "      '10': 'conv3_1',\n",
    "      '19': 'conv4_1',\n",
    "      '21': 'conv4_2',  ## content representation\n",
    "      '28': 'conv5_1'\n",
    "  }\n",
    "\n",
    "  features = {}\n",
    "  x=image\n",
    "  for name, layer in model._modules.items():\n",
    "    x=layer(x)\n",
    "    if name in layers:\n",
    "      features[layers[name]] = x\n",
    "\n",
    "  return features\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "  b, d, h, w = tensor.size()\n",
    "  tensor = tensor.view(b*d, h*w)\n",
    "\n",
    "  return torch.mm(tensor, tensor.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_features=get_features(content_image, model)\n",
    "style_features = get_features(style_image, model)\n",
    "\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "target = content_image.clone().requires_grad_(True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Weight and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_weights= {\n",
    "    'conv1_1': 1.0,\n",
    "    'conv2_1': 0.75,\n",
    "    'conv3_1': 0.6,\n",
    "    'conv4_1': 0.2,\n",
    "    'conv5_1': 0.2\n",
    "}\n",
    "\n",
    "content_weight = 1 #alpha\n",
    "style_weight = 1e6 #beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating and Calculating Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_every = 400\n",
    "optimizer = optim.Adam([target], lr=0.0005)\n",
    "\n",
    "steps = 40000\n",
    "\n",
    "for i in range(1, steps+1):\n",
    "  target_features = get_features(target, model)\n",
    "  content_loss = torch.mean((target_features['conv4_2']- content_features['conv4_2'])**2)\n",
    "\n",
    "  style_loss = 0\n",
    "\n",
    "  for layer in style_weights:\n",
    "    target_feature = target_features[layer]\n",
    "    _, d, h,w = target_feature.shape\n",
    "\n",
    "    target_gram = gram_matrix(target_feature)\n",
    "\n",
    "    style_gram = style_grams[layer]\n",
    "\n",
    "    layer_style_loss = style_weights[layer]*torch.mean((target_gram - style_gram)**2)\n",
    "\n",
    "    style_loss += layer_style_loss / (d*h*w)\n",
    "  total_loss = content_weight*content_loss + style_weight*style_loss\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  total_loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  if i% show_every==0:\n",
    "    print(f'Total loss: ${total_loss.item()}')\n",
    "    plt.imshow(im_convert(target))\n",
    "    plt.title(f'Total loss: ${total_loss.item()}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mixed_image.png](mixed_image.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
